{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "conf = None\n",
    "sc = None\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['JAVA_HOME'] = \"C:\\Program Files\\Java\\jdk1.8.0_271\"\n",
    "os.environ['SPARK_HOME'] = \"C:\\devTools\\spark-3.1.2-bin-hadoop3.2\"\n",
    "os.environ['HADOOP_HOME'] = \"C:\\devTools\\spark-3.1.2-bin-hadoop3.2\"\n",
    "os.environ['PYTHONPATH'] = \"%SPARK_HOME%/python;set PYTHONPATH=C:/devTools/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/python/lib/py4j-0.10.9-src.zip;%PYTHONPATH%\"\n",
    "sys.path.append(\"C:\\devTools\\spark-3.1.2-bin-hadoop3.2\\python\")\n",
    "sys.path.append(\"C:\\devTools\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printResults(counter, amount):\n",
    "    print(\"The 24 most common words in file:\")\n",
    "    for word, count in sortedWords.take(amount):\n",
    "        counter+=1\n",
    "        print(\"{}.\\t '{}'\\t {}\".format(counter, word, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 24 most common words in file:\n",
      "1.\t ''\t 517194\n",
      "2.\t 'the'\t 23197\n",
      "3.\t 'I'\t 19540\n",
      "4.\t 'and'\t 18263\n",
      "5.\t 'to'\t 15592\n",
      "6.\t 'of'\t 15507\n",
      "7.\t 'a'\t 12516\n",
      "8.\t 'my'\t 10824\n",
      "9.\t 'in'\t 9565\n",
      "10.\t 'you'\t 9059\n",
      "11.\t 'is'\t 7831\n",
      "12.\t 'that'\t 7521\n",
      "13.\t 'And'\t 7068\n",
      "14.\t 'not'\t 6946\n",
      "15.\t 'with'\t 6718\n",
      "16.\t 'his'\t 6218\n",
      "17.\t 'your'\t 6003\n",
      "18.\t 'be'\t 5991\n",
      "19.\t 'for'\t 5600\n",
      "20.\t 'have'\t 5231\n",
      "21.\t 'it'\t 4903\n",
      "22.\t 'me'\t 4832\n",
      "23.\t 'this'\t 4760\n",
      "24.\t 'he'\t 4546\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\" and sc == None:\n",
    "\n",
    "    # create Spark context with necessary configuration\n",
    "    sc = SparkContext(\"local[*]\", \"PySpark Word Count\")\n",
    "\n",
    "    # read data from text file and split each line into words\n",
    "    words = sc.textFile(\"shakespeare.txt\").flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "    # map the words\n",
    "    wordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "    wordValues = words.countByValue()\n",
    "\n",
    "    # sort by amount\n",
    "    sortedWords = sc.parallelize(wordValues.items()).sortBy(lambda wc: wc[1], ascending=False)\n",
    "\n",
    "    counter = 0\n",
    "    amount = 24\n",
    "    printResults(counter, amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
