{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "conf = None\n",
    "sc = None\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['JAVA_HOME'] = \"C:\\Program Files\\Java\\jdk1.8.0_271\"\n",
    "os.environ['SPARK_HOME'] = \"C:\\devTools\\spark-3.1.2-bin-hadoop3.2\"\n",
    "os.environ['HADOOP_HOME'] = \"C:\\devTools\\spark-3.1.2-bin-hadoop3.2\"\n",
    "os.environ['PYTHONPATH'] = \"%SPARK_HOME%/python;set PYTHONPATH=C:/devTools/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/python/lib/py4j-0.10.9-src.zip;%PYTHONPATH%\"\n",
    "sys.path.append(\"C:\\devTools\\spark-3.1.2-bin-hadoop3.2\\python\")\n",
    "sys.path.append(\"C:\\devTools\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The n most used words in file:\n",
      ": 517194\n",
      "the: 23197\n",
      "I: 19540\n",
      "and: 18263\n",
      "to: 15592\n",
      "of: 15507\n",
      "a: 12516\n",
      "my: 10824\n",
      "in: 9565\n",
      "you: 9059\n",
      "is: 7831\n",
      "that: 7521\n",
      "And: 7068\n",
      "not: 6946\n",
      "with: 6718\n",
      "his: 6218\n",
      "your: 6003\n",
      "be: 5991\n",
      "for: 5600\n",
      "have: 5231\n",
      "it: 4903\n",
      "me: 4832\n",
      "this: 4760\n",
      "he: 4546\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\" and sc == None:\n",
    "\n",
    "    # create Spark context with necessary configuration\n",
    "    sc = SparkContext(\"local[*]\", \"PySpark Word Count\")\n",
    "\n",
    "    # read data from text file and split each line into words\n",
    "    words = sc.textFile(\"shakespeare.txt\").flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "    # map the words\n",
    "    wordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "    wordValues = words.countByValue()\n",
    "\n",
    "    # sort by amount\n",
    "    sortedWords = sc.parallelize(wordValues.items()).sortBy(lambda wc: wc[1], ascending=False)\n",
    "\n",
    "    print(\"The n most used words in file:\")\n",
    "    for word, count in sortedWords.take(24):\n",
    "        print(\"{}: {}\".format(word, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
